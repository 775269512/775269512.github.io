---
layout: post
title: "Grobid从入门到入土"
date: 2019-06-01 
tag: 机器学习
---   


# Grobid从入门到入土

------

实验室老师最近要求我完成一个任务---**提取论文的标题，姓名，摘要，介绍等结构信息，并可以用于搜索系统。**并且给了一个任务文档，用的核心工具便是Grobid。为此，我专门花了一天时间上网搜索了资料。在做的时候遇到了许多问题，网上的相关资料又特别少，所以写这一篇介绍来分享一下做过程中的的各种坑收获。

------
# 1.下载
a. GROBID是一个机器学习库，用于将原始文档（如PDF）解压缩并重新构建为结构化TEI编码文档。Grobid的安装需要用到maven，git等工具，具体方法见上面链接中的文档，grobid的优点是通过机器学习自动生成这些的标签特征性很强并且XML文件层次结构清晰。

**官方文档**（http://grobid.readthedocs.io/en/latest/）

左侧找到install ,点击框选部分，可以来到grobid项目的gayhub页面:
（mac用户用终端直接下载安装）
![img1](/images/posts/grobid/img1.png)
点击release，进入发行版的下载，根据环境选择win或linux的版本：
![img2](/images/posts/grobid/img2.png)
![img3](/images/posts/grobid/img3.png)
等待下载完成，解压后将得到：core是核心程序，里面有各种api的测试文件，home等下说作用（见javaAPI），gradlew是用来开启服务的文件，至于其他的bin,doc应该都懂蛤，不懂的也不影响后续的使用。
**文件目录**
![img4](/images/posts/grobid/img4.png)

-----
# 2.安装
按官网给的方法，需要进入文件目录，然后gradlew clean install：
![img5](/images/posts/grobid/img5.png)

> * 安装Java环境
> * 下载grobid：git clone https://github.com/kermitt2/grobid.git
> * Build grobid：进入grobid根目录，打开控制台，执行gradlew clean install


实现的时候将会下一堆的jar包，很慢很慢，对于加快的方法，兴许挂一个VPN可能会有用（但我挂的时候没感觉多快）

安装结束后会有一个原谅色的大大的：
![img6](/images/posts/grobid/img6.png)

------
# 3.启动服务
按官网的说明,在目录里用命令行输入：gradlew run就可以在本地8070端口启动服务：
![img7](/images/posts/grobid/img7.png)

很好，我们来**试试**（我电脑很慢？？？）：
![img8](/images/posts/grobid/img8.png)
不，注意官方文档这一句话！！**（Gradle进程将挂起88％，这是正常的，因为运行的Web服务与Gradle共享相同的JVM）**

#### 接下来在浏览器输入http://localhost:8070就可以在浏览器查看grobid服务：
![img9](/images/posts/grobid/img9.png)
至此，grobid下载安装启动就完成了，接下来介绍下使用：

--------

# 4.使用

## 1) 网页GUI使用
TEI是最主要的使用方式，我们可以用它来处理pdf文档

PDF是用于处理带标注的PDF文档

Patent用于处理专利相关的文档？（这个官网的介绍有点看不懂，贴在下面，英语好的可以自己看）
    
    On the console, the RESTful API can be tested under the TEI tab for service returning a TEI document, under the PDF tab for services returning annotations relative to PDF or an annotated PDF and under the Patent tab for patent-related services:
    
在PDF下我们可以选择几种模式：
![img10](/images/posts/grobid/img10.png)
> * header：处理论文的头部，处理论文从论文名到摘要的内容
> * fulltext:处理全文档，包括头部和引用
> * reference:处理论文的引用部分的内容

一般我们使用fulltext，因为它包含其他两个部分的内容，而在这个选项下又有其他选项，就按默认的来.

接下来可以点击select file选择pdf文件，然后点击submint，等待几十秒，就可以在下方看到输出，你也可以将xml文件下载下来：
![img11](/images/posts/grobid/img11.png)
好了，至此，我就可以把老师给的有**1G大小的文档**一篇一篇的下载转换为xml。估计一周就可以搞完了。（干！ 我还有期末考试啊！）

#### 但这种方式无法批量处理文档，怎么办呢？

## 2) javaAPI的使用
嘿嘿，还有一手：
官网细心地给出了两个example，大家闲得无聊的可以下下来看看:
https://github.com/kermitt2/grobid-example
https://github.com/kermitt2/grobid-test-ant

经过我的踩的一个个坑，得出了以下调用方式（以grobid-example为例）：
![img12](/images/posts/grobid/img12.png)
![img13](/images/posts/grobid/img13.png)
![img14](/images/posts/grobid/img14.png)
![img15](/images/posts/grobid/img15.png)

但是在我使用的时候，一度陷入了量子状态，一会儿可以一会儿又报错的，而且报错的地方又在十分内部的地方，百度谷歌都找不到解决方法（心态炸了有没有！！！），于是我弃疗了。

## 3）curl的使用
在我的JAVA程序进入了心态血崩，我投向了官网给出的第二种方法：curl，看起来好简单的！curl只需要一行代码诶！
![img16](/images/posts/grobid/img16.png)

curl是什么呢？就是在使用命令行来访问网站，win10貌似自带，cd进目录，执行：
![img17](/images/posts/grobid/img17.png)

![img18](/images/posts/grobid/img18.png)
以上红框部分是cd进目录并执行curl的演示，白框和蓝款是回车后的结果，其中，白框是curl向本地服务器8070端口发送的数据，蓝框是本地服务器返回的结果，包括头部和数据。这个要怎么批量处理嘞？
等等！**貌似python有个os库有系统相关的东西**，果然——os.popen("cmd命令")可以用于调用cmd,在参数中传入cmd命令，接受cmd的结果那我们就用**python大法**处理pdf文件吧,如下图，再写个保存和for循环就可以了，美滋滋啊！

废话不多说，打开我的大JB（JetBrains）：
![img19](/images/posts/grobid/img19.png)
艹！，编码问题！这是为什么呢？百度后得知win10中国区的cmd默认编码是gbk，而当文档中出现其他gbk外的西文字符时，将出现这个问题，知道问题所在后，我用修改注册表的方式修改了cmd的编码方式，但这时候又出现了没有被服务器拒绝的问题：

找了半天找不到解决方法，但在寻找的过程中，我发现了curl貌似是个比较过时的东西，现在大家用的都是
貌似是**urllib和request**了，urllib？request?爬虫老朋友啊！！

## 4）webAPI调用
python大法好

python大法好

python大法好

官方文档给出了api的使用方法：

**Clients for GROBID Web Services**
    
    We provide clients written in Python, Java, node.js using the GROBID PDF-to-TEI conversion Web services for parallel batch processing:

* Python GROBID client
* Java GROBID client
* Node.js GROBID client

    All these clients will take advantage of the multi-threading for scaling PDF batch processing. As a consequence, they will be much more efficient than the batch command lines (which use only one thread) and should be prefered.
    
所以我们知道可以向http://localhost:8070/api/processFulltextDocument用post方法将文档发送过去，然后得到xml的response，在post的数据端中有一个必填的input，值为文档二进制内容，选填的我们可以不管.

核心代码如下：
```
import os
import requests
from time import sleep

filenmae = r"C:\Users\77526\Documents\Tencent Files\775269512\FileRecv\08566160.pdf"

#请求URL
url = "http://localhost:8070/api/processFulltextDocument"
#构造请求数据
params = dict(input=open(filenmae,'rb'))
# post给服务器，并得到返回
tei = requests.post(url,files =params,timeout = 300)
#保存文件
fh = "./1.xml"
with open(fh,"w",encoding='utf8') as f:
    f.write(tei.text)
```

接下来**循环文档**就好了！

## 5）完成老师的任务
 转换完成后，使用python的beautifulsoup模块将XML进行解析，通过特定的函数来查找、提取所需要的标签和标签中的内容，例如：提取标题就使用find函数找到<title>标签、提取姓名则寻找<persName>标签，利用get_text()函数来提取标签中的字符串。

#### 不过这都是后话了...
-----

# 5.尚未解决的问题
* 此方法只是单独的一个解析和提取论文信息的程序，还需要进行下一步工作以便于将其整合到搜索系统中。
* 对于abstract，introduction，reference这些论文基本内容很好提取，但是论文中的具体的内容和二级标题难以提取。


-------
* 作者：Zichuan Liu
* 邮箱：775269512@qq.com
* 博客：https://775269512.github.io/