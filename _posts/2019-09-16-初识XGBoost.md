---
layout: post
title: 集成学习方法：初始XGBoost
date: 2019-09-16
tags: Machine_Learning
---

# XGBoost介绍
在竞赛题中经常会用到XGBoost算法，用这个算法通常会使我们模型的准确率有一个较大的提升。既然它效果这么好，那么它从头到尾做了一件什么事呢？以及它是怎么样去做的呢？

我们先来直观的理解一下什么是XGBoost。XGBoost算法是和决策树算法联系到一起的。决策树算法在我的另一篇博客中讲过了.

## 集成算法思想

在决策树中，我们知道一个样本往左边分或者往右边分，最终到达叶子结点，这样来进行一个分类任务。 其实也可以做回归任务。



![img](/images/posts/matlab/34.png)

看上面一个图例左边：有5个样本，现在想看下这5个人愿不愿意去玩游戏，这5个人现在都分到了叶子结点里面，**对不同的叶子结点分配不同的权重项**，正数代表这个人愿意去玩游戏，负数代表这个人不愿意去玩游戏。所以我们可以通过叶子结点和权值的结合，来综合的评判当前这个人到底是愿意还是不愿意去玩游戏。上面「tree1」那个小男孩它所处的叶子结点的权值是+2（可以理解为得分）。

用单个决策树好像效果一般来说不是太好，或者说可能会太绝对。通常我们会用一种集成的方法，**就是一棵树效果可能不太好，用两棵树呢？**

看图例右边的「tree2」，它和左边的不同在于它使用了另外的指标，出了年龄和性别，还可以考虑使用电脑频率这个划分属性。通过这两棵树共同帮我们决策当前这个人愿不愿意玩游戏，小男孩在「tree1」的权值是+2，在「tree2」的权值是+0.9， 所以小男孩最终的权值是+2.9（可以理解为得分是+2.9）。老爷爷最终的权值也是通过一样的过程得到的。

所以说，我们通常在做分类或者回归任务的时候，需要想一想一旦选择用一个分类器可能表达效果并不是很好，那么就要考虑用这样一个集成的思想。上面的图例只是举了两个分类器，其实还可以有**更多更复杂的弱分类器，一起组合成一个强分类器。**

 

## XGBoost基本思想

XGBoost的集成表示是什么？怎么预测？求最优解的目标是什么？看下图的说明你就能一目了然。

![img](/images/posts/matlab/35.png)

在XGBoost里，**每棵树是一个一个往里面加的**，每加一个都是希望效果能够提升，下图就是XGBoost这个集成的表示（核心）。

![img](/images/posts/matlab/36.png)

一开始树是0，然后往里面加树，相当于多了一个函数，再加第二棵树，相当于又多了一个函数...等等，这里需要**保证加入新的函数能够提升整体对表达效果**。提升表达效果的意思就是说加上新的树之后，目标函数（就是损失）的值会下降。

如果叶子结点的个数太多，那么**过拟合**的风险会越大，所以这里要限制叶子结点的个数，所以在原来目标函数里要加上一个**惩罚项「omega(ft)」**。


这里举个简单的例子看看惩罚项「omega(ft)」是如何计算的：
![img](/images/posts/matlab/38.png)

一共3个叶子结点，权重分别是2，0.1，-1，带入「omega(ft)」中就得到上面图例的式子，惩罚力度和「lambda」的值人为给定。

-----
**XGBoost算法完整的目标函数**见下面这个公式，它由自身的损失函数和正则化惩罚项「omega(ft)」相加而成。

![img](/images/posts/matlab/39.png)
关于目标函数的**推导本文章不作详细介绍，详见之后的文章**。过程就是：给目标函数对权重求偏导，得到一个能够使目标函数最小的权重，把这个权重代回到目标函数中，这个回代结果就是求解后的最小目标函数值，如下：

![img](/images/posts/matlab/37.png)

其中第三个式子中的一阶导二阶导的**梯度数据**都是可以算出来的，只要指定了主函数中的两个参数，这就是一个确定的值。下面给出一个直观的例子来看下这个过程。

![img](/images/posts/matlab/40.png)

（这里多说一句：Obj代表了当我们指定一个树的结构的时候，在目标上最多会减少多少，我们可以把它叫做结构分数，**这个分数越小越好**）

对于每次扩展，我们依旧要**枚举所有可能的方案**。对于某个特定的分割，我们要计算出这个分割的左子树的导数和和右子数导数和之和（就是下图中的第一个红色方框），然后和划分前的进行比较（基于损失，看分割后的损失和分割前的损失有没有发生变化，变化了多少）。遍历所有分割，**选择变化最大的作为最合适的分割**。

![img](/images/posts/matlab/41.png)

------
# XGBoost简单数据代码实现
* 注：以下代码在jupyter notepad上完成

### 导入必要的工具包
```python
# 导入必要的工具包
import xgboost as xgb
# 计算分类正确率
from sklearn.metrics import accuracy_score
```
### 数据介绍
XGBoost可以加载libsvm格式的文本数据，libsvm的文件格式（稀疏特征）如下： 1 101:1.2 102:0.03 0 1:2.1 10001:300 10002:400 ...

每一行表示一个样本，第一行的开头的“1”是样本的标签。“101”和“102”为特征索引，'1.2'和'0.03' 为特征的值。 在两类分类中，用“1”表示正样本，用“0” 表示负样本。也支持[0,1]表示概率用来做标签，表示为正样本的概率。

下面的示例数据需要我们通过一些蘑菇的若干属性判断这个品种是否有毒。 UCI数据描述：http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/ ， 每个样本描述了蘑菇的22个属性，比如形状、气味等等（将22维原始特征用加工后变成了126维特征， 并存为libsvm格式)，然后给出了这个蘑菇是否可食用。其中6513个样本做训练，1611个样本做测试。

XGBoost加载的数据存储在对象DMatrix中 XGBoost自定义了一个数据矩阵类DMatrix，优化了存储和运算速度 DMatrix文档：http://xgboost.readthedocs.io/en/latest/python/python_api.html

```python
# read in data，数据在xgboost安装的路径下的demo目录,现在我们将其copy到当前代码下的data目录
my_workpath = './data/'
dtrain = xgb.DMatrix(my_workpath + 'agaricus.txt.train')
dtest = xgb.DMatrix(my_workpath + 'agaricus.txt.test')
```
查看数据情况
```python
dtrain.num_col()#127L
```
```python
dtrain.num_row()#6513L
```
```python
dtest.num_row()#1611L
```

### 训练参数设置

* max_depth： 树的最大深度。缺省值为6，取值范围为：[1,∞]
* eta：为了防止过拟合，更新过程中用到的收缩步长。在每次提升计算之后，算法会直接获得新特征的权重。 eta通过缩减特征的权重使提升计算过程更加保守。缺省值为0.3，取值范围为：[0,1]
* silent：取0时表示打印出运行时信息，取1时表示以缄默方式运行，不打印运行时信息。缺省值为0
* objective： 定义学习任务及相应的学习目标，“binary:logistic” 表示二分类的逻辑回归问题，输出为概率。
* 其他参数取默认值，暂不介绍。

```python
# specify parameters via map
param = {'max_depth':2, 'eta':1, 'silent':0, 'objective':'binary:logistic' }
print(param)    #{'objective': 'binary:logistic', 'eta': 1, 'max_depth': 2, 'silent': 0}
```

### 训练模型
有了参数列表和数据就可以训练模型了

```python
# 设置boosting迭代计算次数
num_round = 2#设置树的个数，这里为2棵，之后会更多
import time

starttime = time.clock()
bst = xgb.train(param, dtrain, num_round)
endtime = time.clock()

print (endtime - starttime)#计算模型训练时间0.022666s
```
查看模型在训练集上的分类性能

XGBoost预测的输出是概率。这里蘑菇分类是一个二类分类问题，输出值是样本为第一类的概率。 我们需要将概率值转换为0或1。
```python
train_preds = bst.predict(dtrain)
train_predictions = [round(value) for value in train_preds]#概率四舍五入

y_train = dtrain.get_label()#[0 0 1 0 ....]目标值
train_accuracy = accuracy_score(y_train, train_predictions)#计算准确率
print ("Train Accuary: %.2f%%" % (train_accuracy * 100.0))#Train Accuary: 97.77%
```

### 测试集评价
模型训练好后，可以用训练好的模型对测试数据进行预测

检查模型在测试集上的正确率 XGBoost预测的输出是概率，输出值是样本为第一
类的概率。我们需要将概率值转换为0或1。
```python
# make prediction
preds = bst.predict(dtest)
predictions = [round(value) for value in preds]#概率四舍五入

y_test = dtest.get_label()
test_accuracy = accuracy_score(y_test, predictions)
print("Test Accuracy: %.2f%%" % (test_accuracy * 100.0))#Test Accuracy: 97.83%
```

### 模型可视化（可略）
调用XGBoost工具包中的plot_tree，在显示 要可视化模型需要安装graphviz软件包（请见预备0），plot_tree（）的三个参数：
* 模型
* 树的索引，从0开始
* 显示方向，缺省为竖直，‘LR'是水平方向

```python
from matplotlib import pyplot
import graphviz
xgb.plot_tree(bst, num_trees=0, rankdir= 'LR' )
pyplot.show()

#xgb.plot_tree(bst,num_trees=1, rankdir= 'LR' )
#pyplot.show()
#xgb.to_graphviz(bst,num_trees=0)
#xgb.to_graphviz(bst,num_trees=1)
```

--------